Fast, Reliable AI Inference at Scale | Together AI Serverless inference Run inference withÂ an API call Use our fast API to run inference on 200+ open-source models powered by the Together Inference Stack. Start building now Contact us Llama 3.3 70B DeepSeek V3 FLUX.1 [dev] Qwen 2.5-Coder 32B Qwen QwQ-32B Llama 3.2 11B Free Llama 3.2 90B Llama 4 Maverick Llama 4 Scout Arcee AI AFM-4.5B Qwen 2.5 72B DeepSeek R1 FLUX.1 [schnell] Free FLUX1.1 [pro] Qwen2.5-VL 72B Gemma 3 (27B) Mistral Small 3 Cartesia Sonic-2 200+ generative AI models Build with open-source and specialized multimodal models for chat, images, code, and more. Migrate from closed models with OpenAI-compatible APIs. All Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Try now together.ai Chat Kimi K2 Thinking 1T parameter MoE thinking agent achieving 44.9% HLE with tools, 60.2% BrowseComp, native INT4 quantization, 256K context, and stable 200-300 step tool orchestration TRYâ¨THIS MODEL New Chat Qwen3 235B A22B Instruct 2507 FP8 235B MoE model with 22B activation featuring enhanced instruction following, reasoning, and 262K context for cost-efficient high-throughput inference. TRYâ¨THIS MODEL Chat gpt-oss-120B 120B parameters, 128K context, reasoning with chain-of-thought, MoE architecture, Apache 2.0 license TRYâ¨THIS MODEL New Chat DeepSeek-V3.1 671B parameters (37B activated), 128K context, hybrid thinking/non-thinking modes, advanced tool calling, agent capabilities TRYâ¨THIS MODEL New Chat Apriel-1.5-15b-Thinker 15B multimodal reasoning model, 131K context, scores 52 on AA Intelligence Index, competitive with models 10x larger, text-SFT only TRYâ¨THIS MODEL Free Image Gemini Flash Image 2.5 (Nano Banana) Efficient multimodal image generation with Gemini's advanced capabilities. TRYâ¨THIS MODEL New Video Sora 2 Pro Professional-grade video AI with extended 16-second generation, superior quality, no watermarks, and enhanced physics simulation for commercial use. TRYâ¨THIS MODEL New Chat GLM-4.6 Top-tier coding and agentic model rivaling Claude Sonnet 4 with 48.6% win rate, 200K context, superior frontend generation, and 30% improved efficiency at 357B parameters TRYâ¨THIS MODEL New Chat Gemma 3 27B Lightweight model with vision-language input, multilingual support, visual reasoning, and top-tier performance per size. TRYâ¨THIS MODEL Chat Llama 4 Maverick SOTA 128-expert MoE powerhouse for multilingual image/text understanding, creative writing, and enterprise-scale applications. TRYâ¨THIS MODEL Image FLUX.1 Kontext [dev] In-context image generation and editing model using both text and image inputs. TRYâ¨THIS MODEL Transcribe Whisper Large v3 High-performance speech-to-text model delivering transcription 15x faster than OpenAI with support for 1GB+ files, 50+ languages, and production-ready infrastructure. TRYâ¨THIS MODEL Code Qwen3-Coder 480B A35B Instruct 480B-parameter MoE coding model with 35B active, 256K context, and agentic performance rivaling Claude Sonnet on complex development tasks. TRYâ¨THIS MODEL Chat Kimi K2 Instruct-0905 Upgraded state-of-the-art mixture-of-experts agentic intelligence model with 1T parameters, 256K context, and native tool use TRYâ¨THIS MODEL New Video Google Veo 3.0 Latest Veo generation with improved quality, 720p resolution, 8-second duration. TRYâ¨THIS MODEL Chat Mistral Small 3 24B model rivaling GPT-4o mini, and larger models like Llama 3.3 70B. Ideal for chat use cases like customer support, translation and summarization. TRYâ¨THIS MODEL Embeddings GTE ModernBERT base Text embedding and rerank models built on modernBERT, excelling in MTEB, LoCO, and COIR retrieval benchmarks. TRYâ¨THIS MODEL Chat MiniMax M1 40K 456B-parameter hybrid MoE reasoning model with 40K thinking budget, lightning attention, and 1M token context for efficient reasoning and problem-solving tasks. TRYâ¨THIS MODEL Code DeepSeek-V3.2-Exp 685B parameter experimental model with 128K context, DeepSeek Sparse Attention for long-context efficiency, performance parity with V3.1-Terminus TRYâ¨THIS MODEL New Image Google Imagen 4.0 Ultra Premium image generation with maximum quality and detail for professional applications. TRYâ¨THIS MODEL Video Sora 2 Advanced video generation with improved physics, native audio, up to 20-second clips at 1080p, and multi-shot consistency. TRYâ¨THIS MODEL Audio Orpheus TTS State-of-the-art speech-LLM built on Llama-3B delivering human-like speech with zero-shot voice cloning, guided emotion control, ~200ms streaming latency, and superior quality to closed-source models TRYâ¨THIS MODEL Orpheus TTS Transcribe Voxtral-Mini-3B-2507 3B parameter audio-language model with 32K context handling 40-minute audio, state-of-the-art transcription across 8 languages, built-in Q&A, and voice-to-function calling TRYâ¨THIS MODEL New Transcribe Whisper Large v3 Turbo (Streaming) Realtime speech-to-text with 1.55B parameters over WebSocket, delivering industry-leading 2488ms response times with optimized VAD and turn detection for natural voice agent conversations. TRYâ¨THIS MODEL Rerank Mxbai Rerank Large V2 1.5B-parameter RL-trained reranking model achieving state-of-the-art accuracy across 100+ languages with 8K context, outperforming Cohere and Voyage. TRYâ¨THIS MODEL Next 1 / 7 Explore all models Inference that is fast, simple, and scales as you grow. Fast Run leading open-source models like DeepSeek and Llama on the fastest inference engine available, up to 4x faster than vLLM. â Outperforms Amazon Bedrock, and Azure AI by over 2x. â COST-EFFICIENT Together Inference is 11x lower cost than GPT-4o when using Llama 3.3 70B and 9x lower cost than OpenAI o1 when using DeepSeek-R1. Our optimizations bring you the best performance at the lowest cost. scalable We obsess over system optimization and scaling so you donât have to. As your application grows, capacity is automatically added to meet your API request volume. Run fast Serverless Endpoints for leading open-source models Access 200+ models through serverless endpoints â including DeepSeek, Llama, Qwen, Mistral, FLUX, and many others. Endpoints are OpenAI compatible. Try the best open-source models for Chat, Image, Audio, Vision, Code/Language, Rerank, and Embeddings with the fastest serverless API. Use the fastest inference API available today - 10x faster than DeepSeek R1's own API - with top marks for running inference on all open-source models. import os, requests url = "https://api.together.xyz/v1/chat/completions" payload = { "model": "deepseek-ai/DeepSeek-R1", "max_tokens": 512, "temperature": 0.7, "top_p": 0.7, "top_k": 50, "repetition_penalty": 1 } headers = { "accept": "application/json", "content-type": "application/json", "Authorization": "Bearer TOGETHER_API_KEY" } response = requests.post(url, json=payload, headers=headers) print(response.text) # Sign up to get your API key here: https://api.together.ai/ # Documentation for API usage: https://docs.together.ai/ Try now Watch demo together endpoints create \ --model mistralai/Mixtral-8x7B-Instruct-v0.1 \ --gpu h100 \ --min-replicas 1 \ --max-replicas 8 \ --gpu-count 2 \ --display-name "My Endpoint" \ --wait Deploy on Dedicated Endpoints for unmatched price-performance at scale Customize your single-tenant deployment powered by the latest NVIDIA GPU hardware and optimized by innovations like speculative decoding. Leverage flexible vertical and horizontal scaling options to ensure your deployment always meets traffic demands, even during spikes. Use our CLI or API to create, update and delete your dedicated GPU instances. Deploy a model Powered by the TogetherÂ Inference Stack Built by AI researchers for AI innovators, the Together Inference Stack gives you Â the fastest NVIDIA GPUs running our proprietary Inference Engine, optimized by Together Kernel Collection and customized to your traffic. Fast Inference Engine Running on the latestÂ NVIDIA GPUs with custom optimizations, our Inference Engine offers inference that's 4x faster than vLLM. Together Kernel Collection The Together Kernel Collection, fromÂ ourÂ Chief Scientist and FlashAttention creatorÂ Tri Dao, provides up to 10% faster training and 75% faster inference. Customized to Your Traffic Profile Together AIâs Research team will fine-tune and optimize your deployment using our proprietary optimizations such as custom speculators. ""Together AI offers optimized performance at scale, and at a lower cost than closed-source providers â all while maintaining strict privacy standards." â - Vineet Khosla, CTO, The Washington Post Performance You get more tokens per second, higher throughput and lower time to first token. And, all these efficiencies mean we can provide you compute at a lower cost. SPEED RELATIVE TO VLLM 4 x FASTER LLAMA 3.3 70B DEPLOYED ON 2x h100 6100 TOKENS/SEC COST RELATIVE TO GPT-4o 11 x lower cost Enterprise-grade security and data privacy We take security and compliance seriously, with strict data privacy controls to keep your information protected. Your data and models remain fully under your ownership, safeguarded by robust security measures. Learn more Flexible deployment options Together Cloud Get started quickly with fully managed serverless endpoints with pay-per-token pricing Dedicated GPU endpoints with autoscaling for consistent performance TRYÂ NOW Your Cloud Dedicated serverless deployments in your cloud provider VPC deployment available for additional security Use your existing cloud spend CONTACTÂ SALES Together GPU Clusters For large-scale inference workloads or foundation model training NVIDIA H100 and H200s clusters with Infiniband and NVLink Available with Together Training and Inference Engines for up to 25% faster training and 75% faster inference than PyTorch CONTACTÂ SALES Customer Stories See how we support leading teams around the world. Our customers are creating innovative generative AI applications, faster. How Hedra Scales Viral AI Video Generation with 60% Cost Savings When Standard Inference Frameworks Failed, Together AI Enabled 5x Performance Breakthrough From AWS to Together Dedicated Endpoints: Arcee AI's journey to greater inference flexibility Run inference with the best price-performance at scale Explore our model library Subscribe to newsletter Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Products Solutions Research Blog About Careers Pricing Contact Support Status Trust Center Â© 2025 San Francisco, CA 94114 Consent Preferences Cookie Policy Privacy policy Terms of service