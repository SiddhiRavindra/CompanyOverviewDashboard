Open-source Research: Models, Datasets, and Optimizations | Together AI Research Advancing the open-source AI frontier Our research team contributes cutting-edge models, datasets, and optimizations to the open-source community. Research AdapTive-LeArning Speculator System (ATLAS): A New Paradigm in LLM Inference via Runtime-Learning Accelerators ByÂ Junxiong Wang, Shirley Wu, Zelei Shao, Vikranth Srivatsa, Jue Wang, Roy Yuan, Qingyang Wu, Alpay Ariyak, Rupert Wu, Wai Tong Chung, Chenfeng Xu, Yonatan Oren, Pragaash Ponnusamy, Yineng Zhang, Avner May, Leon Song, Tri Dao, Percy Liang, Ce Zhang, Ben Athiwaratkun ã» October 10, 2025 Research Large Reasoning Models Fail to Follow Instructions During Reasoning: A Benchmark Study ByÂ Yongchan Kwon, Shang Zhu, Federico Bianchi, Kaitlyn Zhou, James Zou ã» October 22, 2025 Research Multimodal Document RAG with Llama 3.2 Vision and ColQwen2 ByÂ Zain Hasan ã» October 8, 2024 Research Direct Preference Optimization: A Technical Deep Dive ByÂ Ivan Provilkov, Zain Hasan, Max Ryabinin ã» April 17, 2025 Research Minions: embracing small LMs, shifting compute on-device, and cutting cloud costs in the process ByÂ Avanika Narayan*, Dan Biderman*, Sabri Eyuboglu*, Avner May, Scott Linderman, James Zou, Christopher RÃ© ã» February 25, 2025 Research Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas ByÂ Austin Silveria, Soham Govande, Dan Fu ã» April 21, 2025 Research Long Context Fine-Tuning: A Technical Deep Dive ByÂ George Grigorev, Zain Hasan, Max Ryabinin ã» November 25, 2024 RedPajama RedPajama provides a set of leading open-source foundation models built on the largest-ever open pre-training dataset. 01 RedPajama-Data-30T The largest open-source pre-training dataset, used by over 500 leading generative AI models. This dataset and the open research approach used to create the RedPajama models is helping to advance the frontier of open-source AI. Learn more 02 RedPajama-7B A suite of fully open-source base, instruction-tuned, and chat models. â The instruct model is the highest scoring open model on HELM benchmarks, making it ideal for a wide range of tasks. It outperforms LLaMA-7B and state-of-the-art open models such as Falcon-7B (Base and Instruct) and MPT-7B (Base and Instruct) on HELM by 2-9 points. Learn more 03 RedPajama-3B The smaller RedPajama model is ideally suited for running on the edge, with support for running on iPhones, Android smartphones, Raspberry pi, and other devices. Learn more Innovations Innovations that make training and inference faster, more scalable, and reliable. 01 FlashAttention-2 This update to FlashAttention is now broadly used by all transformer models, speeds up training and fine-tuning of LLMs by up to 9x and achieves 72% model FLOPs utilization for training on NVIDIA A100s. Learn more 02 Sub-quadratic model architectures In collaboration with Hazy Research , we are actively working on the next core architecture for generative AI models that will provide much faster performance, with longer context. Research in this area includes Hyena, Monarch Mixer, and FlashConv. Learn more 03 Cocktail SGD One of the key challenges in training generative AI models is networking. To enable faster, more reliable training that can run in a distributed environment, we created Cocktail SGD â a set of optimizations that reduces network communication by 117x. Learn more Research Read the latest research from our team and academic partners Oct 22 Â ã»Â 2025 Research Large Reasoning Models Fail to Follow Instructions During Reasoning: A Benchmark Study ByÂ Yongchan Kwon, Shang Zhu, Federico Bianchi, Kaitlyn Zhou, James Zou Oct 10 Â ã»Â 2025 Research AdapTive-LeArning Speculator System (ATLAS): A New Paradigm in LLM Inference via Runtime-Learning Accelerators ByÂ Junxiong Wang, Shirley Wu, Zelei Shao, Vikranth Srivatsa, Jue Wang, Roy Yuan, Qingyang Wu, Alpay Ariyak, Rupert Wu, Wai Tong Chung, Chenfeng Xu, Yonatan Oren, Pragaash Ponnusamy, Yineng Zhang, Avner May, Leon Song, Tri Dao, Percy Liang, Ce Zhang, Ben Athiwaratkun Aug 21 Â ã»Â 2025 Research How Together AI Uses AI Agents to Automate Complex Engineering Tasks: Lessons from Developing Efficient LLM Inference Systems ByÂ Shang Zhu, Federico Bianchi, Wai Tong Chung, Zain Hasan, Rupert Wu, Ce Zhang, James Zou, Ben Athiwaratkun Jul 17 Â ã»Â 2025 Research Back to The Future: Evaluating AI Agents on Predicting Future Events ByÂ Federico Bianchi, Junlin Wang, Zain Hasan, Shang Zhu, Roy Yuan, ClÃ©mentine Fourrier, James Zou Jul 2 Â ã»Â 2025 Research DeepSWE: Training a Fully Open-sourced, State-of-the-Art Coding Agent by Scaling RL ByÂ Michael Luo*, Naman Jain*, Jaskirat Singh*, Sijun Tan*, Ameen Patel*, Qingyang Wu*, Alpay Ariyak*, Colin Cai*, Tarun Venkat, Shang Zhu, Ben Athiwaratkun, Manan Roongta, Ce Zhang, Li Erran Li, Raluca Ada Popa, Koushik Sen, Ion Stoica 1 ... 1 / 12 Start building yours here â Subscribe to newsletter Thank you! Your submission has been received! Oops! Something went wrong while submitting the form. Products Solutions Research Blog About Careers Pricing Contact Support Status Trust Center Â© 2025 San Francisco, CA 94114 Consent Preferences Cookie Policy Privacy policy Terms of service