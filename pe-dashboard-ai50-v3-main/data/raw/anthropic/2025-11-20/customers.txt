Research \ Anthropic Skip to main content Skip to footer Try Claude Research Our research teams investigate the safety, inner workings, and societal impacts of AI models – so that artificial intelligence has a positive impact as it becomes increasingly capable. Research teams: Interpretability Alignment Societal Impacts Interpretability The mission of the Interpretability team is to discover and understand how large language models work internally, as a foundation for AI safety and positive outcomes. Alignment The Alignment team works to understand the risks of AI models and develop ways to ensure that future ones remain helpful, honest, and harmless. Societal Impacts Working closely with the Anthropic Policy and Safeguards teams, Societal Impacts is a technical research team that explores how AI is used in the real world. Frontier Red Team The Frontier Red Team analyzes the implications of frontier AI models for cybersecurity, biosecurity, and autonomous systems. Project Fetch: Can Claude train a robot dog? Policy Nov 12, 2025 How much does Claude help people program robots? To find out, two teams of Anthropic staff raced to teach quadruped robots to fetch beach balls. The AI-assisted team completed tasks faster and was the only group to make real progress toward full autonomy. Interpretability Oct 29, 2025 Signs of introspection in large language models Can Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models. Interpretability Mar 27, 2025 Tracing the thoughts of a large language model Circuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another. Alignment Feb 3, 2025 Constitutional Classifiers: Defending against universal jailbreaks These classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered. Alignment Dec 18, 2024 Alignment faking in large language models This paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences. Publications Search Date Category Title Nov 12, 2025 Policy Project Fetch: Can Claude train a robot dog? Nov 4, 2025 Alignment Commitments on model deprecation and preservation Oct 29, 2025 Interpretability Signs of introspection in large language models Oct 14, 2025 Policy Preparing for AI’s economic impact: exploring policy responses Oct 9, 2025 Alignment A small number of samples can poison LLMs of any size Oct 6, 2025 Alignment Petri: An open-source auditing tool to accelerate AI safety research Oct 3, 2025 Policy Building AI for cyber defenders Sep 15, 2025 Economic Research Anthropic Economic Index report: Uneven geographic and enterprise AI adoption Sep 15, 2025 Economic Research Anthropic Economic Index: Tracking AI’s role in the US and global economy Aug 27, 2025 Societal Impacts Anthropic Education Report: How educators use Claude See more Join the Research team See open roles